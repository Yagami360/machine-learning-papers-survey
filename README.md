# MachineLearning-Papers_Survey
機械学習関連の論文 Survey 用レポジトリです。<br>
論文まとめ記事は、[Issues](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues) に記載しています。進捗は、[Projects ページ](https://github.com/Yagami360/MachineLearning-Papers_Survey/projects/1) で管理しています。

## ■ 構成

### ◎ 基礎系（基礎モデル）

<details>
<summary>CNN</summary>

- [[ResNet] Deep Residual Learning for Image Recognition](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_NN_Note.md#ResNet%EF%BC%88%E6%AE%8B%E5%B7%AE%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%EF%BC%89)
- [Spatial Transformer Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/48)
</details>

<details>
<summary>GCN</summary>

- [Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_NN_Note.md#Convolutional_Neural_Networks_on_Graphs_with_Fast_Localized_Spectral_Filtering)
- [Semi-Supervised Classification with Graph Convolutional Networks](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_NN_Note.md#Semi-Supervised_Classification_with_Graph_Convolutional_Networks)
- [[R-GCN] Relational Graph Convolutional Network](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_NN_Note.md#R-GCN%EF%BC%88%E3%82%B0%E3%83%A9%E3%83%95%E3%83%95%E3%83%BC%E3%83%AA%E3%82%A8%E5%A4%89%E6%8F%9B%E3%82%92%E7%94%A8%E3%81%84%E3%81%AA%E3%81%84%E3%82%B0%E3%83%A9%E3%83%95%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%EF%BC%89)
</details>

<details>
<summary>RNN</summary>

- [[RNN] Recursive Neural Network](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_NN_Note.md#%E3%83%AA%E3%82%AB%E3%83%AC%E3%83%B3%E3%83%88%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF-rnn--recursive-neural-network%E9%9A%8E%E5%B1%A4%E5%9E%8B%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF)
- [[LSTM] long short-term memory](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_NN_Note.md#%E9%95%B7%E7%9F%AD%E6%9C%9F%E8%A8%98%E6%86%B6lstm-long-short-term-memory%E3%83%A2%E3%83%87%E3%83%AB)
- [[GRU] gated recurrent unit](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_NN_Note.md#gru-gated-recurrent-unit)
</details>

<details>
<summary>Transformer</summary>

- [[Transformer] Attention Is All You Need](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/115)
- [[Vision Transformer] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/116)
- [TransGAN: Two Transformers Can Make One Strong GAN](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/117)
</details>

<details>
<summary>VAE</summary>

- [[VAE] Auto-Encoding Variational Bayes](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#VAE)
- [[VQ-VAE] Neural Discrete Representation Learning](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/23)
- [β-VAE: LEARNING BASIC VISUAL CONCEPTS WITH A CONSTRAINED VARIATIONAL FRAMEWORK](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/27)
</details>

<details>
<summary>GANs</summary>

- [[GAN] Generative Adversarial Networks](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#GAN)
- [[DCGAN] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#DCGAN)
- [[cGAN] Conditional Generative Adversarial Nets](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#ConditionalGAN%EF%BC%88cGAN%EF%BC%89)
- [[WGAN] Wasserstein GAN](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#WGAN)
- [[WGAN-gp] improved Training of Wasserstein GANs](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/25)
- [SAGAN [Self-Attention Generative Adversarial Networks]](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#SAGAN)
- [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/26)
- [[RSGAN,RGAN,RaGAN] The relativistic discriminator: a key element missing from standard GAN](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/51)
- [GAN-Tree: An Incrementally Learned Hierarchical Generative Framework for Multi-Modal Data Distributions](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/19)
- [A U-Net Based Discriminator for Generative Adversarial Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/110)
</details>

<details>
<summary>Flow-based generative model</summary>

- [NICE: NON-LINEAR INDEPENDENT COMPONENTS ESTIMATION](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/10)
- [Real NVP [Density estimation using Real NVP]](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/11)
- [Glow [Generative Flow with Invertible 1×1 Convolutions]](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/13)
- [i-ResNets [Invertible residual networks]](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/14)
- [Residual Flows for Invertible Generative Modeling](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/15)
</details>

<details>
<summary>Autoregressive Models</summary>

- [[PixelRNN, PixelCNN] Pixel Recurrent Neural Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/24)
</details>

<details>
<summary>meta-learning, few-shot learning</summary>

- [MAML:Model Agnostic Meta-Learning for Fast Adaption](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/39)
</details>

<details>
<summary>Neural-ODE</summary>

- [[Neural-ODE] Neural Ordinary Differential Equations](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/21)
- [Augmented Neural ODEs](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/37)
</details>

<details>
<summary>Neural Processes</summary>

- [Conditional Neural Processes](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/38)
- [Neural Processes](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/44)
</details>

<br>

### ◎ アプリケーション系（CV）

<details>
<summary>Image Classification</summary>

- xxx
</details>

<details>
<summary>Semantic Segmentation</summary>

- [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#UNet)
- [[PSPNet] Pyramid Scene Parsing Network](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/69)
- [Pyramid Attention Network for Semantic Segmentation](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/63)
- [[DeepLab v3+] Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/68)
- [Concurrent Spatial and Channel ‘Squeeze & Excitation’ in Fully Convolutional Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/62)
- [Hypercolumns for Object Segmentation and Fine-grained Localization](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/64)
- [Tversky loss function for image segmentation using 3D fully convolutional deep networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/67)
- [Boundary loss for highly unbalanced segmentation](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/77)
- Human Parsing
    - [[JPPNet] Look into Person: Joint Body Parsing & Pose Estimation Network and A New Benchmark](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/70)
    - [[CE2P] Devil in the Details: Towards Accurate Single and Multiple Human Parsing](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/72)
    - [Graphonomy: Universal Human Parsing via Graph Transfer Learning](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/8)
    - [Hierarchical Human Parsing with Typed Part-Relation Reasoning](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/79)
    - [[CorrPM] Correlating Edge, Pose with Parsing](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/74)
</details>

<details>
<summary>Object Detection</summary>

- [Fast R-CNN](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/75)
- [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/76)
- [Focal Loss for Dense Object Detection](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/66)
</details>

<details>
<summary>Instance Segmentation</summary>

- [Mask R-CNN](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/65)
- Human Parsing
    - [Parsing R-CNN for Instance-Level Human Analysis](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/73)
</details>

<details>
<summary>Pose Estimation</summary>

- [DensePose: Dense Human Pose Estimation in the Wild](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/50)
</details>

<details>
<summary>Image Registration / geometric matching</summary>

- [Convolutional neural network architecture for geometric matching](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/36)
</details>

<details>
<summary>image-to-image</summary>

- [[pix2pix] Image-to-Image Translation with Conditional Adversarial Networks](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#pix2pix)
- [[pix2pix-HD] High-Resolution_Image_Synthesis_and_Semantic_Manipulation_with_Conditional_GANs](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/18)
- [[CycleGAN] Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#CycleGAN)
- [[StarGAN] StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#StarGAN)
- [[SPADE] Semantic Image Synthesis with Spatially-Adaptive Normalization](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/7)
- [[Neural Collage] Spatially Controllable Image Synthesis with Internal Representation Collaging](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/22)
- [Recapture as You Want](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/78)
- [Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/104)
- [[Impersonator++] Liquid Warping GAN with Attention: A Unified Framework for Human Image Synthesis](https://github.com/Yagami360/MachineLearning-Papers_Survey_Private/issues/5)
- [Focal Frequency Loss for Generative Models](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/114)
- few-shot learning
    - [SinGAN: Learning a Generative Model from a Single Natural Image](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/40)
    - [[DeepSIM] Deep Single Image Manipulation](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/107)
</details>

<details>
<summary>noize-to-image</summary>

- [[PGGAN] Progressive Growing of GANs for Improved Quality, Stability, and Variation](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#ProgressiveGAN%EF%BC%88PGGAN%EF%BC%89)
- [[StyleGAN] A Style-Based Generator Architecture for Generative Adversarial Networks](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#StyleGAN)
- [[StyleGAN2] Analyzing and Improving the Image Quality of StyleGAN](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/52)
- few-shot learning
    - [Training Generative Adversarial Networks with Limited Data / StyleGAN2 with adaptive discriminator augmentation (ADA)](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/108)
    - [Data-Efficient GANs with DiffAugment](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/109)
    - [[Lightweight GAN] Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/113)

</details>

<details>
<summary>Inpainting</summary>

- [[Deepfillv2] Free-Form Image Inpainting with Gated Convolution](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/29)
- [Pluralistic Image Completion](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/6)
- [Boundless: Generative Adversarial Networks for Image Extension](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/28)
</details>

<details>
<summary>Person Image Generation</summary>

- [Pose Guided Person Image Generation](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/49)
- [Disentangled Person Image Generation](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/41)
- [[Soft-Gated Warping-GAN] Soft-Gated Warping-GAN for Pose-Guided Person Image Synthesis](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/35)
</details>

<!--
<details>
<summary>顔特化系（Face Swap, etc）</summary>

- [[GANimation] GANimation: Anatomically-aware Facial Animation from a Single Image](https://github.com/Yagami360/My_NoteBook/blob/master/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6/%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92_%E7%94%9F%E6%88%90%E3%83%A2%E3%83%87%E3%83%AB.md#GANimation)
- [On Face Segmentation, Face Swapping, and Face Perception](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/3)
- [Face Swapping: Realistic Image Synthesis Based on Facial Landmarks Alignment](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/4)
- [FaceShifter: Towards High Fidelity And Occlusion Aware Face Swapping](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/106)
</details>

<details>
<summary>Virtual Try-On</summary>

- [VITON: An Image-based Virtual Try-on Network](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/30)
- [[CP-VTON] Toward Characteristic-Preserving Image-based Virtual Try-On Network](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/53)
- [[MG-VTON] Towards_Multi-pose_Guided_Virtual_Try-on_Network](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/5)
- [[WUTON] End-to-End Learning of Geometric Deformations of Feature Maps for Virtual Try-On](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/12)
- [Generating High-Resolution Fashion Model Images Wearing Custom Outfits](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/16)
- [Poly-GAN: Multi-Conditioned GAN for Fashion Synthesis](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/17)
- [Virtually Trying on New Clothing with Arbitrary Poses](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/31)
- [FW-GAN: Flow-navigated Warping GAN for Video Virtual Try-on](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/42)
- [Robust Cloth Warping via Multi-Scale Patch Adversarial Loss for Virtual Try-On Framework](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/45)
- [ClothFlow: A Flow-Based Model for Clothed Person Generation](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/46)
- [SieveNet: A Unified Framework for Robust Image-Based Virtual Try-On](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/54)
- [GarmentGAN: Photo-realistic Adversarial Fashion Transfer](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/56)
- [[ACGPN] Towards Photo-Realistic Virtual Try-On by Adaptively Generating↔Preserving Image Content](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/55)
- [Toward Accurate and Realistic Virtual Try-on Through Shape Matching and Multiple Warps](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/57)
- [DEEP LEARNING APPROACHES FOR ATTRIBUTE MANIPULATION AND TEXT-TO-IMAGE SYNTHESIS / Chapter4 : Attribute Manipulation Generative Adversarial Networks for Image-to-Image Translation](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/58)
- [FashionOn: Semantic-guided Image-based Virtual Try-on with Detailed Human and Clothing Information](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/61)
- [SwapNet: Image Based Garment Transfer](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/105)
- [LGVTON: A Landmark Guided Approach to Virtual Try-On](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/59)
- [[Outfit-VITON] Image Based Virtual Try-On Network From Unpaired Data](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/71)
- [Do Not Mask What You Do Not Need to Mask: a Parser-Free Virtual Try-On](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/80)
- [CP-VTON+: Clothing Shape and Texture Preserving Image-Based Virtual Try-On](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/82)
- [3D Reconstruction of Clothes using a Human Body Model and its Application to Image-based Virtual Try-On](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/88)
</details>
-->

<details>
<summary>recommendation</summary>

- [ViBE: Dressing for Diverse Body Shapes](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/60)
</details>

<details>
<summary>text-to-image</summary>

- [[StackGAN] Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Network](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/9)
</details>

<details>
<summary>Optical Flow</summary>

- [FlowNet: Learning Optical Flow with Convolutional Networks](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/43)
- [View Synthesis by Appearance Flow](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/47)
</details>

<details>
<summary>3D Reconstruction</summary>

- param-to-3D / parametric 3D models
    - [SMPL: A skinned multi-person linear model ](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/86)
    - [[CAPE] Learning to Dress 3D People in Generative Clothing](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/93)
    - [TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape and Garment Style](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/91)
    - [SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/84)
- image-to-3D / image-based 3D Reconstruction 
    - none-parametric 3D models
        - [Mesh R-CNN](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/83)
        - [Occupancy Networks: Learning 3D Reconstruction in Function Space](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/90)
        - [PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/99)
        - [PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/94)
        - [NormalGAN: Learning Detailed 3D Human from a Single RGB-D Image](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/95)
        - using templete mesh
            - [3D Virtual Garment Modeling from RGB Images](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/81)
    - parametric 3D models
        - [[HMR] End-to-end Recovery of Human Shape and Pose](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/98)
        - [Multi-Garment Net: Learning to Dress 3D People from Images](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/87)
        - [Deep Fashion3D: A Dataset and Benchmark for 3D Garment Reconstruction from Single Images](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/89)
        - [BCNet: Learning Body and Cloth Shape from A Single Image](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/92)
        - [ExPose: Monocular Expressive Body Regression through Body-Driven Attention](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/97)
        - [I2L-MeshNet: Image-to-Lixel Prediction Network for Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/100)
- video-to-3D
    - [TexMesh: Reconstructing Detailed Human Texture and Geometry from RGB-D Video](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/96)
- texture mapping
    - [360-Degree Textures of People in Clothing from a Single Image](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/103)
    - [Learning to Transfer Texture from Clothing Images to 3D Humans](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/85)
- camera localization
    - [PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/101)
    - [Geometry-Aware Learning of Maps for Camera Localization](https://github.com/Yagami360/MachineLearning-Papers_Survey/issues/102)
</details>

<details>
<summary>Others</summary>

- xxx
</details>

<br>

### ◎ 理論系

<details>
<summary>xxx</summary>

- xxx

</details>

## ■ 機械学習系の論文調査や論文の読み方

- 論文調査
    - その論文の発展版を探すには、Google Scholer でその論文の被引用論文を探すのが手っ取り早い。
    - 基本的には、その論文の被引用数が多いものが注目の論文。但し最新の論文は当然被引用数は少なくなる。
    - 著者の所属組織も優れた論文であるか判断材料になり得る。例えば、GAFA や NVIDIA, Adobe などの論文は、優れた論文である可能性が高い
    - 基本的には、最新の論文ほど優れた実験結果を出しているので、Google Scholer で出来るだけ公開日時の最新のものを探す。
    - 論文の公式実装がないものは、自前実装の再現コストが高いので、公式実装があるものを優先的に探す。
    - 論文の公式実装の有無は、papers with code で確認できる。
    - github 上に論文公式実装がある場合は、スター数も優れた論文であるかの判断材料になり得る。
    - 探している分野の論文が１つ見つかれば、その論文を papers with code で検索する。その論文の "Tasks" が割り振られている場合は、その Tasks の名前からその分野の最新論文を調べることが出来る。

- 論文の読み方
    - 基本的に、「Abstract」→「Introduction」→「Conclusion」→「何をしたかの詳細」→「Experiments」→「Related Work」項目の順に読むのが効率的。Related Work は最悪飛ばしても良い。
    - Abstract や Introduction 内の "In this paper ...", "In this work ..." という文面や、Introduction の最後にある "the contributions of this paper ... as follows." という文面には注目。
    - その論文の未解決課題や問題点は、論文中の Future work や Conculution に述べられていることが多い。或いは、その論文を引用している論文の Related Work に述べられていることが多い。


## ■ 論文要約フォーマット（要約バージョン）

```
layout: post

title:  "論文タイトル"

date:   YYYY-MM-DD

categories: CV NLP Others

## 1. どんなもの？

## 2. 先行研究と比べてどこがすごいの？

## 3. 技術や手法の"キモ"はどこにある？

![Figure 1]({{ site.baseurl }}/assets/img/(cv, nlp, others)/(title)/figure1.png)

## 4. どうやって有効だと検証した？

## 5. 議論はあるか？

## 6. 次に読むべき論文はあるか？

### 論文情報・リンク

* [著者，"タイトル，" ジャーナル名，voluem，no.，ページ，年](論文リンク)

```
- [例）先端技術とメディア表現1 #FTMA15](http://www.slideshare.net/Ochyai/1-ftma15) from [Yoichi Ochiai](http://www.slideshare.net/Ochyai)

![](https://raw.githubusercontent.com/shunk031/paper-survey/master/assets/img/FTMA15-1-page-65.png)


## ■ 参考サイト

### ◎ 論文サイト
- [arXiv](https://arxiv.org/)
- [Google Scholer](https://scholar.google.co.jp/schhp?hl=ja&as_sdt=0,5)
- [Microsoft Academic](https://academic.microsoft.com/home)

### ◎ 便利サイト
- [DeepL](https://www.deepl.com/translator)
- [papers with code](https://paperswithcode.com/)
    - 論文実装の有無を確認できる。    
- [Hyper Collocation](https://hypcol.marutank.net/ja/)
- [arXiv Vanity](https://www.arxiv-vanity.com/)

### ◎ その他参考サイト
- [arXivTimes](https://github.com/arXivTimes/arXivTimes)
- [ymym3412/acl-papers](https://github.com/ymym3412/acl-papers)
- [shunk031/paper-survey](https://github.com/shunk031/paper-survey)
